\documentclass[10pt]{amsart}
\usepackage{latexsym}
\usepackage{amscd,amsthm,amssymb,amsfonts,amsmath}
\usepackage{xcolor}
\usepackage{float, graphicx}
\usepackage[matrix,tips,graph,curve]{xy}

\newcommand{\mnote}[1]{${}^*$\marginpar{\footnotesize ${}^*$#1}}
\linespread{1.065}
\setlength{\parskip}{0.5em}

\renewcommand{\labelenumi}{(\alph{enumi})}

\makeatletter

\setlength\@tempdima  {5.5in}
\addtolength\@tempdima {-\textwidth}
\addtolength\hoffset{-0.5\@tempdima}
\setlength{\textwidth}{5.5in}
\setlength{\textheight}{8.75in}
\addtolength\voffset{-0.625in}

\newtheorem{manualtheoreminner}{}
\newenvironment{exercise}[1]{%
        \vspace{10mm}
        \renewcommand\themanualtheoreminner{#1}%
  \manualtheoreminner
}\hrulefill{\endmanualtheoreminner}

\newcommand{\p}{\partial}

\begin{document}
\title{Math 531 Homework 10}
\author{Braden Hoagland}
\date{\today}
\maketitle

\begin{exercise}{Page 274, Ex. 4}
	Let $\mathcal{B}\subset \mathcal{C}([0,1], \mathbb{R})$ be closed, bounded, and equicontinuous. Let $I:\mathcal{B}\to\mathbb{R}$ be defined by $I(f) = \int_{0}^{1} f(x) \;dx$. Show that there is an $f_0 \in \mathcal{B}$ at which the value of $I$ is maximized.
\end{exercise}

We first show that $\mathcal{B}$ is compact. Consider $\mathcal{B}_x = \left\{ f(x) \;|\; f \in \mathcal{B} \right\}$. Since $\mathcal{B} \subset \mathbb{R}$, it is compact if and only if it is closed and bounded. Since $\mathcal{B}$ is bounded, $\mathcal{B}_x$ must also be bounded. Now for fixed $x$, consider $f_n(x) \to f(x)$ in $\mathcal{B}_x$. Since $\mathcal{B}$ is closed, $f$ is in $\mathcal{B}$, so $f(x)$ is in $\mathcal{B}_x$. Thus $\mathcal{B}_x$ is closed. This shows that $\mathcal{B}$ is pointwise compact. Since we are given that it is also closed and equicontinuous, and since $[0,1]$ is compact in $\mathbb{R}$, $\mathcal{B}$ compact by the Arzela-Ascoli theorem.

Now we show that $I$ is continuous. Fix $\varepsilon>0$, and let $f$ and $g$ be functions in $\mathcal{B}$ such that $|f-g| < \varepsilon$, as measured by the supremum norm. Then
\begin{align*}
	|I(f) - I(g)| &= \left| \int_{0}^{1} (f(x)-g(x)) \;dx \right| \\
		      &\leq \int_{0}^{1} |f(x)-g(x)| \;dx \\
		      &< \int_{0}^{1} \varepsilon \;dx \\
		      &= \varepsilon.
\end{align*}
Thus $I$ is continuous.

Since $\mathcal{B}$ is compact and $I$ is continuous, by the minimum-maximum theorem we know that there exists $f_0 \in \mathcal{B}$ such that $I(f_0) = \sup_f I(f)$.

%====================
\begin{exercise}{Page 275, Ex. 5}
	Let the functions $f_n:[a,b]\to\mathbb{R}$ be uniformly bounded continuous functions. Set
	\[
		F_n(x) = \int_{a}^{x} f_n(t) \;dt, \quad a \leq x \leq b.
	\] Prove that $F_n$ has a uniformly convergent subsequence.
\end{exercise}

We will show that the set $\mathcal{B} \doteq \left\{ F_n \right\}_{n=1}^\infty$ is equicontinuous and pointwise bounded, then we can use the same proof as for Corollary 5.6.3 in the textbook to show our desired result.

First we show that $\mathcal{B}$ is equicontinuous. Let $G_n$ be any antiderivative of $f_n$, then $F_n(x) = G_n(x) - G_n(a)$, so $F_n'(x) = f_n(x)$. Then $F_n$ is an antiderivative of $f_n$, so the intermediate value theorem gives us
\[
	|F_n(x) - F_n(y)| = |f_n(c)| |x-y|
\] for some $c \in [x,y]$. Since $\left\{ f_n \right\}$ is uniformly bounded, for all $ n \in \mathbb{N}, x \in [a,b]$, we have $|f_n(x)| \leq M$ for some $M \geq 0$. Thus we have the inequality
\[
	|F_n(x) - F_n(y)| \leq M |x-y|.
\] 
Now fix $\varepsilon>0$. If $|x-y| < \varepsilon/M$, then for all $n \in \mathbb{N}$ and $x,y \in [a,b]$, we have $|F_n(x) - F_n(y)| < \varepsilon$, so $\mathcal{B}$ is equicontinuous.

Now we show that $\mathcal{B}$ is pointwise bounded. Fix $x$, then
\begin{align*}
	|F_n(x)| &= \left| \int_{a}^{x} f_n(t) \;dt \right| \\
		 &\leq \int_{a}^{x} \left| f_n(t) \right| \;dt \\
		 &\leq \int_{a}^{x} M \;dt \\
		 &\leq M (x-a).
\end{align*}
Thus $\mathcal{B}$ is pointwise bounded.

Then by Corollary 5.6.3 in the textbook, every sequence in $\mathcal{B}$ has a uniformly convergent subsequence.

%====================
\begin{exercise}{Page 282, Ex. 4}
	Show that the system of equations
	\begin{align*}
		x_1 &= \frac{1}{4} x_1 - \frac{1}{4} x_2+\frac{2}{15} x_3 + 3 \\
		x_2 &= \frac{1}{4} x_1+\frac{1}{5} x_2+\frac{1}{2} x_3-1 \\
		x_3&= -\frac{1}{4} x_1+\frac{1}{3} x_2-\frac{1}{3} x_3+2
	\end{align*}
	has a unique solution, using the contraction mapping principle. [Hint: Either choose a clever norm on $\mathbb{R}^3$, or estimate using the Schwarz inequality.]
\end{exercise}

The given system defines a map $\Phi: \mathbb{R}^3 \to \mathbb{R}^3$. We will show that $\Phi$ is a contraction in the taxicab norm $\Vert{x}\Vert = \sum_i |x_i|$, from which the result for the usual Euclidean norm follows (since the two norms are equivalent). We have
\begin{align*}
	d(\Phi(x), \Phi(y)) &= \Vert \Phi(x) - \Phi(y) \Vert \\
			    &\leq \left| \frac{1}{4} (x_1-y_1) \right| - \left| \frac{1}{4} (x_2-y_2)\right| + \left| \frac{2}{15} (x_3-y_3) \right| \\
			    &\quad + \left| \frac{1}{4} (x_1-y_1) \right| + \left| \frac{1}{5} (x_2-y_2)\right| + \left|\frac{1}{2} (x_3-y_3) \right| \\
			    &\quad + \left| -\frac{1}{4} (x_1-y_1)\right|+\left|\frac{1}{3} (x_2-y_2)\right|-\left|\frac{1}{3} (x_3-y_3) \right| \\
			    &= \frac{3}{4} |x_1-y_1|+ \frac{47}{60} |x_2-y_2| + \frac{29}{30} |x_3-y_3| \\
			    &\leq \frac{29}{30} \left( |x_1-y_1| + |x_2-y_2| + |x_3-y_3| \right) \\
			    &= \frac{29}{30} d(x,y).
\end{align*} Since $0 < 29/30 < 1$, we have found a $k$ such that $d(\Phi(x), \Phi(y)) \leq kd(x,y)$ for $0 \leq k < 1$. Thus by the contraction mapping principle, this system has a unique solution (i.e. fixed point).

%====================
\begin{exercise}{Page 283, Ex. 8}
	Let $M$ be a compact metric space and $\Phi:M \to M$ be such that $d(\Phi(x),\Phi(y)) < d(x,y)$ for all $x,y \in M, x \neq y$.
	\begin{enumerate}
		\item Show that $\Phi$ has a unique fixed point [Hint: Minimize $d(\Phi(x),y)$.]
		\item Show that \textbf{a} is false if $M$ is not compact (find a counterexample).
	\end{enumerate}
\end{exercise}

\begin{enumerate}
	\item Consider the map $f:M \to \mathbb{R}$ given by $f(x) = d(\Phi(x),x)$. We claim that $f$ is continuous. By the triangle inequality and our assumption on $\Phi$, we have
		\begin{align*}
			d(x,\Phi(x)) &\leq d(x,y) + d(y,\Phi(y)) + d(\Phi(y), \Phi(x)) \\
			d(x,\Phi(x)) - d(y,\Phi(y)) &\leq d(x,y) + d(\Phi(y), \Phi(x)) \\
			f(x) - f(y) &< 2 d(x,y).
		\end{align*}
		Similarly, we also have $f(y) - f(x) < 2 d(x,y)$. Putting these two inequalites together gives
		\[
			|f(x) - f(y)| \leq 2 d(x,y).
		\] Now fix $\varepsilon >0$. When $d(x,y) < \varepsilon/2$, $|f(x)-f(y)| < \varepsilon$, so $f$ is continuous.

		Since $M$ is compact and $f$ is continuous, it attains its infimum, i.e. there exists $x_0 \in M$ such that $f(x_0) = \inf_x f(x) = \inf_x d(x, \Phi(x))$. Denote this infimum by $I$. Consider the case when $I>0$. In this case we have a contradiction, as our assumption on $\Phi$ gives
		\[
			d(\Phi(\Phi(x_0)), \Phi(x_0)) < d(\Phi(x_0), x_0) = I,
		\] which cannot be true if $I$ is an infimum. Thus $I=0$, meaning $d(x_0, \Phi(x_0)) = 0$, so $x_0 = \Phi(x_0)$. Thus $x_0$ is a fixed point of $\Phi$.

		We now show that $x_0$ is a unique fixed point. Let $x_0$ and $x_1$ be distinct fixed points of $\Phi$, then our assumption on $\Phi$ gives
		\begin{align*}
			d(\Phi(x_0), \Phi(x_1)) &< d(x_0, x_1) \\
			d(x_0, x_1) &< d(x_0, x_1).
		\end{align*}
		This is a contradiction, so $x_0$ and $x_1$ must be equal. Thus the fixed point $x_0$ of $\Phi$ is unique.

	\item Let $M= \mathbb{R}$ with the usual metric $d(x,y) = |x-y|$. Consider $\Phi:\mathbb{R} \to \mathbb{R}$ given by $\Phi(x) = \sqrt{x^2 + 2} $. Note that
		\[
		\left| \sqrt{x^2+2} -\sqrt{y^2+2}  \right| < |x-y|
		\] for all $x,y \in \mathbb{R}$, so $\Phi$ satisfies the conditions from part  \textbf{a}.

		If $x_0$ were a fixed point of $\Phi$, then it would satisfy $\sqrt{x_0^2 +2} =x_0$; however, trying to solve this yields
		\begin{align*}
			\sqrt{x_0^2 +2} &= x_0 \\
			x_0^2 + 2 &= x_0^2 \\
			2 &= 0,
		\end{align*}
		so no such $x_0$ can exist. Thus $\Phi$ has no fixed points.
\end{enumerate}

%====================
\begin{exercise}{Page 286, Ex. 3}
	Prove that the set of polynomials in $\mathcal{C}([a,b], \mathbb{R})$ is not open. Can a subset of a metric space ever be \textit{both} open and dense?
\end{exercise}

In the last homework we showed that the sequence of functions $\left\{ f_k \right\}$ given by
\[
f_k = \frac{\sin x}{k} 
\] converges uniformly to the zero function. We claim that this is a sequence of non-polynomial functions. If $(\sin x)/ k$ were a polynomial, then we could write it
 \[
\frac{\sin x}{k} = \alpha_n x^n + \alpha_{n-1} x^{n-1} + \cdots + \alpha_0
\] for some $n \in \mathbb{N}$; however, note that the $n$-th derivative of the RHS is 0 for all $x$ while the $n$-th derivative of the LHS is nonzero when $x$ is nonzero and not a multiple of $2\pi$. Thus $f_k = (\sin x)/k$ is not a polynomial.

Since $f_k$ converges uniformly to the zero function, for all $\varepsilon > 0$, we can find an $f_k$ such that $\sup_x |f_k(x)| < \varepsilon$. Since $0$ is a polynomial and all $f_k$ are non-polynomials, this means the set of polynomials in $\mathcal{C}([a,b], \mathbb{R})$ is not open.

In general, it is possible for a dense subset of a metric space to be open. Consider $\mathbb{R}$ equipped with the usual metric. Then the subset $\mathbb{R} - \{0\}$ is open and dense. It is open since its complement $\{0\}$ is closed, and it is dense since its closure is all of $\mathbb{R}$.

%====================
\begin{exercise}{Page 317, Ex. 11}
	\begin{enumerate}
		\item Must a contraction on any metric space have a fixed point? Discuss.
		\item let $f:X \to X$, where $X$ is a complete metric space (such as $\mathbb{R}$), satisfy \[d(f(x), f(y)) < d(x,y)\] for all $x,y \in X$. Must $f$ have a fixed point? What if $X$ is compact?
	\end{enumerate}
\end{exercise}

\begin{enumerate}
	\item A contraction on a metric space need not have a fixed point. In order to guarantee that a fixed point exists, the metric space needs to be complete.

	Suppose we are working in the space $\mathbb{R} - \left\{ 0 \right\}$, which is not complete. Consider the map $\Phi(x) = x / 2$, which is a contraction since
\[
	d(x,y) = \left| \frac{x}{2} -\frac{y}{2}  \right| = \frac{1}{2} |x-y| = \frac{1}{2} d(x,y).
\] 
However, solving $x = x/2$ yields $x=0$, so the only possible fixed point of $\Phi$ is 0, which is not in our metric space. Thus $\Phi$ has no fixed point.

\item $f$ need not have a fixed point. Consider $f(x) = x + 1/x$ on $[2, \infty)$. For all distinct $x,y \in [2, \infty)$, we have
	\begin{align*}
		d(f(x),f(y)) &= \left| x + \frac{1}{x} - y - \frac{1}{y}  \right| \\
			     &= \left| (x-y) + \left( \frac{y-x}{xy}  \right) \right| \\
			     &= \left| (x-y) \left( 1-\frac{1}{xy}  \right) \right| \\
			     &\leq |x-y| \left| 1- \frac{1}{xy} \right| \\
			     &< |x-y|,
	\end{align*}
	where the last inequality follows from $x$ and $y$ both being greater than or equal to 2. This shows that that $f$ satisfies the given condition; however, solving $x = x + 1/x$ yields
	\begin{align*}
		x &= x + \frac{1}{x} \\
		0 &= \frac{1}{x} \\
		0 &= 1,
	\end{align*}
	so there are no fixed points of $f$.

	When we are working in a \textit{compact} metric space, the given condition is enough to guarantee that $f$ has a unique fixed point. This was proven earlier in Exercise 8 (Page 283).
\end{enumerate}

%====================
\begin{exercise}{Page 322, Ex. 46}
	Let $f(t,x)$ be defined and continuous for $a \leq t \leq b$ and $x \in \mathbb{R}^n$. The purpose of this exercise is to show that the problem $dx/dt = f(t,x), x(a)=x_0$, has a solution on an interval $t \in [a,c]$ for some $c>a$ (it is unique only under more stringent conditions). Perform the operations as follows: Divide $[a,b]$ into $n$ equal parts $t_0=a\dots,t_n=b$, and define a continuous function $x_n$ inductively by
	\[
	\begin{cases}
		x'_n(t) = f(t_i, x_n(t_i)), & t_i < t < t_{i+1},\\
		x_n(a) = x_0.
	\end{cases}
\] Put $\Delta_n(t) = x_n'(t) - f(t,x_n(t))$, so that
\[
	x_n(t) = x_0+\int_{a}^{t} f(s,x_n(s))+\Delta_n(s) \;ds.
\] Use the Arzela-Ascoli theorem to find a convergent subsequence of the $x_n$. Show that the limit satisfies $dx/dt = f(t,x)$ and $x(a)=x_0$.

This method is called \textbf{polygonal approximation}.
\end{exercise}

First we note that $f$ is bounded (by, say, $M$) since it is a continuous function on a compact domain. Additionally, since $|t_i - t| \leq 2/n$ and $f$ is continuous, we know $\Delta_n(s)$ is also always bounded (by, say, $N$). Then
\begin{align*}
	|x_n(t)| &\leq |x_0| + \int_{a}^{t} |M+N| \;ds \\
		 &= |x_0| + (t-a)|M+N|,
\end{align*}
so each $x_n$ is pointwise bounded.

Then by the mean value theorem,
\begin{align*}
	|x_n(t_1) - x_n(t_2)| &= |x_n'(t_3)| |t_1-t_2| \\
			      &= |f(t_i, x_n(t_i))| |t_1-t_2| \\
			      &\leq M |t_1-t_2|.
\end{align*}
Since this Lipschitz property holds for all $n$, we have equicontinuity of the space containing the $x_n$'s.

Now we can apply Corollary 5.6.3 from the textboook to show that $\left\{ x_n \right\}$ has a uniformly convergent subsequence $\left\{ x_{\sigma(n)} \right\}$, and we denote its limit function by $x$. We must now show that its limit satisfies the given ODE.

Since this sequence converges uniformly, it certainly converges pointwise, so consider the sequence $\left\{ x_{\sigma(n)}(a) \right\}$. Since $x_n(a) = x_0$ for all $n$, the limit $x(a)$ of this sequence must also equal $x_0$. Now we must show that the derivative of $x$ is equal to $f(t,x)$.

Since $f$ is continuous and $x_{\sigma(n)}$ converges uniformly to $x$, then $f(s, x_{\sigma(n)}(s))$ converges uniformly to $f(s,x(s))$. Thus $\Delta_{\sigma(n)}(s) = f(t_i, x_{\sigma(n)}(t_i)) - f(t, x_{\sigma(n)}(t))$ converges uniformly to
\[
	\Delta(s) = f(t_i, x(t_i)) - f(t, x(t)).
\] Since $|t - t_i| < 2/n$ by construction, $t_i \to t$ as $n \to \infty$. Since $f$ is continuous this means $f(t_i, x(t_i)) \to f(t, x(t))$. Thus we have
\[
	\Delta(s) = 0.
\] 
Putting this all together, we can write $x(t)$ as
\begin{align*}
	x(t) = \lim_{n\to\infty} x_n(t) &= x_0 + \lim_{n \to \infty} \int_{a}^{t} f(s,x_n(s)) \;ds + \lim_{n \to \infty} \int_{a}^{t} \Delta_n(s) \;ds \\
					     &= x_0 + \int_{a}^{t} f(s,x(s)) \;ds + \int_{a}^{t} 0 \;ds \\
					     &= x_0 + \int_{a}^{t} f(s,x(s)) \;ds.
\end{align*}
And taking its derivative gives
\[
	x'(t) = f(t, x(t)),
\] as desired.

%====================
\begin{exercise}{Page 324, Ex. 58b}
	Prove that if $u_n > 0$, $\frac{u_{n+1}}{u_n} \geq 1 - \frac{1}{n} - \frac{1}{n \log n} $, then $\sum u_n$ diverges.
\end{exercise}

We are given $u_{n+1}\geq (1- 1/n - 1/(n \log n)$, and we can expand this to
\begin{align*}
	u_{n+1} &\geq u_2 \prod_{k=2}^n \left(1-\frac{1}{k} -\frac{1}{k \log k} \right) \\
		&= u_2 \exp\left( \sum_{k=2}^{n} \log\left( 1 -\frac{1}{k} -\frac{1}{k\log k} \right) \right).
\end{align*}

We can expand $\log(1-x)$ into
\[
	\log(1-x) = -\sum_{m=1}^{\infty} \frac{x^m}{m},
\] so we have
\[
	\log\left(1-\frac{1}{k} -\frac{1}{k \log k} \right) = \frac{1}{k} + \frac{1}{k\log k} + \rho(k),
\] 
where
\[
	\rho(k) = -\sum_{m=2}^{\infty} \frac{\left( -\frac{1}{k} -\frac{1}{k\log k}  \right)^m}{m} .
\] 

Thus our bound on $u_{n+1}$ becomes
\begin{align*}
	u_{n+1} &\geq u_2 \exp\left( \sum_{k=2}^{n} \frac{1}{k} + \frac{1}{k\log k} + \rho(k) \right) \\
	&\geq u_2 \exp\left( \sum_{k=2}^{n} \frac{1}{k} \right)  \exp\left( \sum_{k=2}^{n} \frac{1}{k \log k}  \right)  \exp\left( \sum_{k=2}^{n} \rho(k)  \right) \\
	&\geq u_2 \exp\left( \sum_{k=2}^{n} \frac{1}{k} \right)  \exp\left( \sum_{k=2}^{n} \frac{1}{k \log k}  \right)  \prod_{k=2}^n e^\rho(k).
\end{align*}

We will now find lower bounds for each of the exponential terms. The sequence $1/k$ is monotonically decreasing, so
\[
\log n -\log 2 = \int_{2}^{n} \frac{1}{t} \;dt \leq \sum_{k=2}^{n} \frac{1}{k} .
\] 
Similarly, the sequence $1/(k \log k)$ is also monotonically decreasing, so
\[
\log |\log n| - \log |\log 2| = \int_{2}^{n} \frac{1}{k \log k} \;dt \leq \sum_{k=2}^{n} \frac{1}{k \log k} .
\] 
In the case of $k=2$, $\rho(k)$ diverges to $-\infty$, so $e^\rho(2) \geq e^0 = 1$. For $k \geq 3$, $\rho(k)$ converges to 0 from above (as the first term in the summation is positive, the signs of each term alternate, and the absolute value of each subsequent term decreases). So in this case, $e^{\rho(k)} = 0$. Thus we have $e^{\rho(k)} \geq 0$ for all $k \geq 2$.

Using these derived inequalities, our bound on $u_{n+1}$ becomes
\begin{align*}
	u_{n+1} &\geq u_2 \exp\left( \sum_{k=2}^{n} \frac{1}{k} \right)  \exp\left( \sum_{k=2}^{n} \frac{1}{k \log k}  \right) \prod_{k=2}^n e^\rho(k) \\
		&\geq u_2 \exp(\log n - \log 2) \cdot \exp(\log |\log n| - \log |\log 2|) \cdot 1 \cdots 1 \\
		&= \frac{u_2}{2 \log 2} n \log n.
\end{align*}
This grows unbouded as $n \to \infty$, so the series $\sum_n u_n$ must diverge.

\end{document}
