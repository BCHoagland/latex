\documentclass[10pt]{amsart}
\usepackage{latexsym}
\usepackage{amscd,amsthm,amssymb,amsfonts,amsmath}
\usepackage{mdframed}
\usepackage{xcolor}
\usepackage{float, graphicx}
\usepackage[matrix,tips,graph,curve]{xy} 

\newcommand{\mnote}[1]{${}^*$\marginpar{\footnotesize ${}^*$#1}}
\linespread{1.065}
\setlength{\parskip}{0.5em}

\renewcommand{\labelenumi}{(\alph{enumi})}

\makeatletter

\setlength\@tempdima  {5.5in}
\addtolength\@tempdima {-\textwidth}
\addtolength\hoffset{-0.5\@tempdima}
\setlength{\textwidth}{5.5in}
\setlength{\textheight}{8.75in}
\addtolength\voffset{-0.625in}

% custom theorem environment
\newmdtheoremenv{manualexercise}{Exercise}
\newenvironment{exercise}[1]{
        \vspace{10mm}
	\renewcommand\manualexercise{\textbf{#1}}
	\begin{manualexercise}
}{
	\end{manualexercise}
	\vspace{5mm}
}

\newcommand{\p}{\partial}

\begin{document}
\title{Math 531 Homework 11}
\author{Braden Hoagland}
\date{\today}
\maketitle

%====================
\begin{exercise}{Pg. 286, Ex. 2}
	Suppose that $p_n$ is a sequence of polynomials converging uniformly to $f$ on $[0,1]$ and $f$ is \textit{not} a polynomial. Prove that the degrees of the $p_n$ are not bounded. [Hint: An $N$-th degree polynomial $p$ is uniquely determined y its values at $N+1$ points $x_0,\dots,x_N$ via Lagrange's interpolation formula
	\[
		p(x) = \sum_{i=0}^{N} \pi_i(x) \frac{p(x_i)}{\pi_i(x_i)} ,
	\] where $\pi_i(x) = (x-x_0)(x-x_1)\cdots(x-x_N)/(x-x_i)$.]
\end{exercise}

We will prove this statement by contrapositive, i.e. given a sequence of polynomials $p_k$ converging uniformly to some function $f$, we will show that if the $p_k$'s have uniformly bounded degree, then $f$ must be a polynomial. 

Since we are assuming that our sequence of polynomials have uniformly bounded degree, there is some $N$ such that $\deg(p_k) \leq N$ for all $k$. Take an arbitrary collection of distinct points $\mathcal{X} \doteq \left\{ x_0, \dots, x_N \right\} \subset [0,1]$.

We claim that $p_k$ converges to the polynomial uniquely defined by the $N+1$ points $f(x_0), \dots f(x_N)$. We denote this polynomial by $f_p$. Fix $\varepsilon>0$, then we use Lagrange's interpolation formula to get
\begin{align*}
	|f_p(x) - p_k(x)| &= \left| \sum_{i=0}^{N} \frac{\pi_i(x)}{\pi_i(x_i)} (f(x_i)-p_k(x_i)) \right| \\
			  &\leq \sum_{i=0}^{N} \left| \frac{\pi_i(x)}{\pi_i(x_i)} \right| \left| f(x_i)-p_k(x_i) \right|.
\end{align*}

Note that $|x-x_i| \leq 1$ for all $i$ since $p$ and $f$ are defined on $[0,1]$. Additionally, since $\mathcal{X}$ is finite, we can define the minimum distance bewteen points in $\mathcal{X}$ as $d \doteq \min_{i,j} |x_i - x_j|$, from which we clearly have $|x_i-x_j|\geq d$ for all $i\neq j$. This means we have
\[
	\left| \frac{\pi_i(x)}{\pi_i(x_i)}  \right| \leq \frac{1}{d^N}.
\] Thus the distance between $f_p$ and $p_k$ is bounded by
\[
	|f_p(x)-p_k(x)| \leq \frac{1}{d^N} \sum_{i=0}^{N} | f(x_i)-p_k(x_i)|.
\] Now since $p_k$ converges uniformly to $f$, we can find a $K$ such that
\[
|f(x) - p_k(x)| < \frac{d^N \varepsilon}{N+1}
\] when $k > K$. Thus for $k>K$, we have
\begin{align*}
	|f_p(x)-p_k(x)| &< \frac{1}{d^N} \sum_{i=0}^{N} \frac{d^N \varepsilon}{N+1} \\
			&= \frac{d^N}{d^N} \frac{N+1}{N+1} \varepsilon \\
			&= \varepsilon.
\end{align*}
Thus $p_k$ converges to the polynomial $f_p$. We have shown that the contrapositive of the desired implication is true, so we are done.

%====================
\begin{exercise}{Pg. 286, Ex. 4}
	Consider the set of all polynomials $p(x,y)$ in two variables $x,y \in [0,1] \times [0,1]$. Prove that this set is dense in $\mathcal{C}([0,1] \times [0,1], \mathbb{R})$.
\end{exercise}

We will show this using the Stone-Weierstrass theorem. Denote the set of polynomials of the two variables $x$ and $y$ by
\[
	\mathcal{P}(x,y) \doteq \left\{ \text{ all functions of the form } \sum_{i=1}^n a_i x^{b_i}y^{c_i} \;|\; a_i \in \mathbb{R} \text{ and } b_i, c_i, n \in \mathbb{Z} \right\}.
\] The set $[0,1]\times[0,1]$ is compact in $\mathbb{R}^2$ since it is closed and bounded, and the set of two-variable polynomials on $[0,1]\times[0,1]$ is a subset of $\mathcal{C}([0,1]\times[0,1], \mathbb{R})$, so we need only show the three main conditions of the Stone-Weierstrass Theorem to conclude that $\mathcal{P}(x,y)$ is dense in $\mathcal{C}([0,1]\times[0,1], \mathbb{R})$.

First we show that $\mathcal{P}(x,y)$ is an algebra. Suppose we have $p_1, p_2 \in \mathcal{P}(x,y)$, where $p_1 = \sum_{i=1}^{n} a_i x^{b_i}y^{c_i}$ and $p_2 = \sum_{i=1}^{m} \alpha_i x^{\beta_i}y^{\gamma_i}$. The product of these two polynomials will have $n+m$ terms, the coefficients will be of the form $a_i \alpha_j$, and the $x$ and $y$ exponents will be of the form $b_i \beta_j$ and $c_i \gamma_j$, respectively. Thus $p_1 p_2 \in \mathcal{P}(x,y)$. The sum $p_1 + p_2$ is similarly in $\mathcal{P}(x,y)$. Finally, for constant $\rho$, $\rho p_1$ has form $\sum_{i=1}^{n} \rho a_i x^{b_i}y^{c_i}$, so it too is in $\mathcal{P}(x,y)$. Thus $\mathcal{P}(x,y)$ is an algebra.

Now we show $1 \in \mathcal{P}(x,y)$. Let $n = 1, a_1 = 1, b_1 = 0$, and $c_1=0$, then \[\sum_{i=1}^{n} a_i x^{b_i} y^{c_i} = 1,\] so $1 \in \mathcal{P}(x,y)$.

Now we show that $\mathcal{P}(x,y)$ separates points. If $(x_1,y_1) \neq (x_2,y_2)$, then either $x_1\neq x_2$ or $y_1 \neq y_2$ (or both simultaneously, which does not merit its own case since it is superceded by the two given cases). If $x_1 \neq x_2$, then for $p(x, y) = x$, we have $p(x_1,y_1) \neq p(x_2,y_2)$. Similarly, if $y_1 \neq y_2$, then for $p(x,y) = y$, we have $p(x_1,y_1)\neq p(x_2,y_2)$.

Thus by the Stone-Weierstrass theorem, $\mathcal{P}(x,y)$ is dense in $\mathcal{C}([0,1]\times[0,1], \mathbb{R})$.

%====================
\newpage
\begin{exercise}{Pg. 286, Ex. 5}
	Consider the set of all functions on $[0,1]$ of the form
	\[
		h(x) = \sum_{j=1}^{n} a_j e^{b_j x}, \quad \text{where } a_j,b_j \in \mathbb{R}.
	\] Is this set dense in $\mathcal{C}([0,1], \mathbb{R})$?
\end{exercise}

We will show that the set of all $h(x)$ is dense in $\mathcal{C}([0,1], \mathbb{R})$ using the Stone-Weierstrass theorem. Let $\mathcal{H}$ denote the set of all possible $h(x)$.

First we show that $\mathcal{H}$ is an algebra. Suppose we have $h_1(x) = \sum_{j=1}^{n} a_j e^{b_j x}$ and $h_2(x) = \sum_{j=1}^{m} \alpha_j e^{\beta_j x}$. Since $e^{b x} e^{\beta x} = e^{(b + \beta) x}$, the product $h_1(x) h_2(x)$ is in $\mathcal{H}$. The sum $h_1(x) + h_2(x)$ is more straightforwardly in $\mathcal{H}$, as the sum is just the concatenation of the two individual summations. Finally, given constant $\rho$, $\rho h_1(x) = \sum_{j=1}^{n} (\rho a_j) e^{b_j x}$ is clearly in $\mathcal{H}$. Thus $\mathcal{H}$ is an algebra.

Now we show that $1 \in \mathcal{H}$. Let $n=1, a_1=1$, and $b_1=0$, then
\[
\sum_{j=1}^{n} a_j e^{b_j x} = 1,
\] so $1 \in \mathcal{H}$.

Now we show that $\mathcal{H}$ separates points. Suppose $x \neq y$, then the function $h(x)=e^x \in \mathcal{H}$ yields $e^x \neq e^y$ since it is strictly monotonically increasing, so $\mathcal{H}$ separates points.

Thus by the Stone-Weierstrass theorem, $\mathcal{H}$ is dense in $\mathcal{C}([0,1], \mathbb{R})$.

%====================
\begin{exercise}{Pg. 322, Ex. 51}
	Consider a double series
	\[
		\sum_{m,n=0}^{\infty} a_{mn}, \quad \text{where } a_{mn}\in \mathbb{R}, \quad m,n = 0,1,2,\dots.
	\] Say that it \textbf{converges to} $S$ if for any $\varepsilon>0$, there is an $N$ such that $n,m > N$ implies
	\[
	\left| \sum_{k,l=0}^{m,n} a_{kl}-S \right| < \varepsilon.
	\] Define \textbf{absolute convergence} and prove that if $\sum_{m,n=0}^{\infty} a_{nm}$ is absolutely convergent, then the sum can be rearranged as follows:
	\[
		\sum_{m,n=0}^{\infty} a_{nm} = \sum_{n=0}^{\infty} \left( \sum_{m=0}^{\infty} a_{nm} \right) = \sum_{m=0}^{\infty} \left( \sum_{n=0}^{\infty} a_{nm} \right).
	\] Interpret this result in terms of summing entries in an infinite matrix by rows and columns.
\end{exercise}

We say a double series $\sum_{k,l=0}^{\infty} a_{mn}$ is \textbf{absolutely convergent} if for any $\varepsilon>0$, there is an $N$ such that
\[
\left| S - \sum_{k,l=0}^{m,n} |a_{mn}| \right| < \varepsilon
\] when $m,n > N$.

Let $\sum_{m,n}a_{mn}$ be such an absolutely convergent double series, and let
\[
s_{mn} \doteq \sum_{k,l=0}^{m,n} a_{kl} = \sum_{k=0}^{m} \sum_{l=0}^{n} a_{kl} = \sum_{l=0}^{n} \sum_{k=0}^{m} a_{kl}
\] denote the partial sums of the double series (note that we can exchange the summations in this way since each summation has a finite number of terms).

Since $\sum_{k,l=0}^{m,n} |a_{kl}|$ converges, we know that $\sum_{k,l=0}^{m,n} a_{kl}$ converges, so $s_{mn}$ converges as well. Let $S \doteq \lim_{n \to \infty, m \to \infty} s_{mn}$, then for all $\varepsilon>0$, there is an $N$ such that $|s_{mn}-S|<\varepsilon$ when $m,n > N$. We will now show that both desired rearrangements of our double series converge to this $S$.

Fix $m$, then consider the subseries $\sum_{n=0}^{\infty} a_{mn}$. Since the full series is absolutely convergent, any of its subsequences converge. Thus $\sum_{n=0}^{\infty} a_{mn}$ must converge. Denote its limit by $b_m$, then we claim that $\sum_{m=0}^{\infty} b_m$ converges to $S$. Fix $\varepsilon>0$, then we have
\begin{align*}
	\left|S-\sum_{k=0}^{m} b_k\right| &= \left| S - \sum_{k=0}^{m} b_k + \sum_{k=0}^{m} \sum_{l=0}^{n} a_{kl} - \sum_{k=0}^{m} \sum_{l=0}^{n} a_{kl} \right| \\
					  &\leq \left| S - \sum_{k=0}^{m} \sum_{l=0}^{n} a_{kl} \right| + \left| \sum_{k=0}^{m} \left( \sum_{l=0}^{n} a_{kl} - b_k \right) \right|.
\end{align*}
Since $\sum_k \sum_l a_{kl}$ converges to $S$ by definition, we can find $N$ such that the first absolute value term is less than $\varepsilon/2$ when $m,n > N_1$. Additionally, for fixed $m$ and $k$, since $\sum_{l}a_{kl}$ converges to $b_k$, we can find $N_k$ such that $| \sum_k a_{kl} - b_k | < \varepsilon/(2m)$ when $n > N_k$. Thus if we take $m> N$ and $n$ such that
\[
	n > \max\{ N, N_0, N_1,\dots, N_m \},
\] we have
\begin{align*}
	\left|S-\sum_{k=0}^{m} b_k\right| &\leq \frac{\varepsilon}{2} + \sum_{k=0}^{m} \left| \frac{\varepsilon}{2m}  \right| \\
					  &= \frac{\varepsilon}{2} + \frac{\varepsilon}{2} \\
					  &= \varepsilon.
\end{align*}
Thus $\sum_k b_k$ converges to $S$, so
\[
	S = \sum_{k,l=0}^{\infty} a_{kl} = \sum_{m=0}^{\infty} \left( \sum_{n=0}^{\infty} a_{mn} \right).
\] By an analogous argument, we can show
\[
	S = \sum_{k,l=0}^{\infty} a_{kl} = \sum_{n=0}^{\infty} \left( \sum_{m=0}^{\infty} a_{mn} \right).
\] In terms of summing the entries of an infinite matrix, this result means that we can either
\begin{enumerate}
	\item sum each row, then add each row sum; or
	\item sum each column, then add each column sum;
\end{enumerate}
and the final sum of each will be the same.

%====================
\begin{exercise}{Pg. 324, Ex. 59}
	\begin{enumerate}
		\item Let $p>1$ with $1/p+1/q = 1$. For $a,b,t>0$, prove that
			\[
			ab \leq \frac{a^p t^p}{p} +\frac{b^qt^{-q}}{q} 
		\] and that $ab$ is the minimum value of the right side (One way to prove this is to use elementary calculus).

		\item Prove \textbf{HÃ¶lder's inequality:} If $a_k,b_k \geq 0$, $p>1$, and $1/p + 1/q=1$, then
			\[
				\sum_{k=1}^{n} a_kb_k \leq \left( \sum_{k=1}^{n} a_k^p \right)^{1/p}\left( \sum_{k=1}^{n} b_k^q \right)^{1/q}.
			\] [Hint: Imitate the proof of the Cauchy-Schwarz inequality, using part \textbf{a}.]

		\item Prove \textbf{Minkowski's inequality:} If $a_k,b_k \geq 0$ and $p>1$, then
			\[
				\left( \sum_{k=1}^{n} (a_k + b_k)^p \right)^{1/p}\leq \left( \sum_{k=1}^{n} a_k^p \right)^{1/p}+\left( \sum_{k=1}^{n} b_k^p \right)^{1/p}.
			\] 
	\end{enumerate}
\end{exercise}

\begin{enumerate}
	\item
		We need only show that $ab$ is the minimum value of the given expression, as the inequality follows from it. Fix $a,b,p$, and $q$, then consider
		\[
			f(t) = \frac{a^p t^p}{p} + \frac{b^q t^{-q}}{q} .
		\] Its derivative is
		\[
			f'(t) = a^pt^{p-1} - b^q t^{-(q+1)}.
		\] We know that if $f(t)$ achieves a local minimum or maximum at some $t^*$, then $f(t^*) = 0$. This gives us a necessary condition for finding the global minimum of $f$. Setting $f'(t)=0$ and solving for $t$ yields
		\[
			t = \left( \frac{b^q}{a^p}  \right)^{\frac{1}{p+q} } = \left( \frac{b^q}{a^p}  \right)^{\frac{1}{pq} },
		\] 
		where the second equality follows from
		\[
		\frac{1}{p} + \frac{1}{q} =1 \implies p+q = pq.
		\] 
		Denote this value of $t$ by $t^*$, then the value of $f$ at this point is
		\begin{align*}
			f(t^*) &= \frac{a^p \frac{b}{a^{p/q}} }{p} + \frac{b^q \frac{a}{b^{q/p}} }{q} \\
			       &= \frac{a^{p-p/q}b}{p} + \frac{b^{q-q/p}a}{q}.
		\end{align*}
		Now since $p+q=pq$, we have
		\[
		p - \frac{p}{q} = \frac{pq-p}{q} = \frac{p+q-p}{q} =1.
	\] Similarly, $q - q/p=1$. Thus we can simplify our expression for $f(t^*)$ to 
	\[
		f(t^*) = \frac{ab}{p} + \frac{ba}{q} = ab \left( \frac{1}{p} +\frac{1}{q}  \right)= ab.
	\] Thus the only possible extrumum of $f$ is the value $ab$. To check that this is a minimum, we can use the second derivative test. We have
	\[
		f''(t) = (p-1) a^p t^{p-2}+ (q+1) b^q t^{-(q+2)}.
	\] Since $a,b,t>0$ and $p>1$, this will always be positive. Thus $ab$ is in fact the minimum value of $f$.

\item Let $\tilde{a}\doteq \left( \sum_{k=1}^{n} a_k^p\right)^{1/p}$ and $\tilde{b} \doteq \left( \sum_{k=1}^{n} b_k^q \right)^{1/q}$. Then our goal is to show \[
\sum_{k=1}^{n} a_k b_k \leq \tilde{a} \tilde{b}.
\] If $\tilde{a}=0$, then since each $a_k$ is non-negative, each $a_k$ must be 0, so the inequality is trivial. The case is similar if $\tilde{b}=0$. Thus if either is 0, the inequality holds (it is actually an equality). We now assume that neither is 0.

Since neither $\tilde{a}$ nor $\tilde{b}$ is 0, our desired inequality is equivalent to \[\sum_{k=1}^{n} \frac{a_k}{\tilde{a}} \frac{b_k}{\tilde{b}} \leq 1.\] Since $a_k/\tilde{a}, b_k/\tilde{b}> 0$ for each $k$, by part \textbf{a} we have
\begin{align*}
	\sum_{k=1}^{n} \frac{a_k}{\tilde{a}} \frac{b_k}{\tilde{b}} &\leq \sum_{k=1}^{n} \left( \frac{ak^p t^p}{\tilde{a}^p p} + \frac{b_k^q t^{-q}}{\tilde{b}^q q}  \right) \\
								   &= \frac{t^p}{p \tilde{a}^p} \sum_{k=1}^{n} a_k^p + \frac{t^{-q}}{q \tilde{b}^q} \sum_{k=1}^{n} b_k^q \\
								   &= \frac{t^p \sum_{k=1}^{n} a_k^p}{p \sum_{k=1}^{n} a_k^p} + \frac{t^{-q} \sum_{k=1}^{n} b_k^q}{q \sum_{k=1}^{n} b_k^q} \\
								   &= \frac{t^p}{p} + \frac{t^{-q}}{q}
\end{align*}
for any $t>0$. Since this holds for any $t$, it certainly holds for $t=1$. Thus our inequality is
\[
\sum_{k=1}^{n} \frac{a_k}{\tilde{a}} \frac{b_k}{\tilde{b}} \leq \frac{1}{p} + \frac{1}{q} = 1,
\] and we are done.

\item By part \textbf{b}, we have
	\begin{align*}
		\sum_{k=1}^{n} (a_k+b_k)^p &= \sum_{k=1}^{n} (a_k+b_k)^{p-1}a_k + \sum_{k=1}^{n} (a_k+b_k)^{p-1} b_k \\
					   &\leq \left( \sum_{k=1}^{n} (a_k+b_k)^{q(p-1)} \right)^{1/q} \left[ \left( \sum_{k=1}^{n} a_k^p \right)^{1/p}+ \left( \sum_{k=1}^{n} b_k^p \right)^{1/p} \right].
	\end{align*}
	But $q(p-1) = pq-q = p+q-q = 1$, so this becomes
	\begin{align*}
		\sum_{k=1}^{n} (a_k+b_k)^p &\leq \left( \sum_{k=1}^{n} (a_k+b_k)^{p} \right)^{1/q} \left[ \left( \sum_{k=1}^{n} a_k^p \right)^{1/p}+ \left( \sum_{k=1}^{n} b_k^p \right)^{1/p} \right] \\
		\left( \sum_{k=1}^{n} (a_k+b_k)^{p} \right)^{\color{blue}1 - 1/q} &\leq \left( \sum_{k=1}^{n} a_k^p \right)^{1/p}+ \left( \sum_{k=1}^{n} b_k^p \right)^{1/p} \\
		\left( \sum_{k=1}^{n} (a_k+b_k)^{p} \right)^{\color{blue}1/p} &\leq \left( \sum_{k=1}^{n} a_k^p \right)^{1/p}+ \left( \sum_{k=1}^{n} b_k^p \right)^{1/p},
	\end{align*}
	which is the desired result.
\end{enumerate}

%====================
\begin{exercise}{Pg. 334, Ex. 3}
	Let $L$ be a linear map of $\mathbb{R}^n\to\mathbb{R}^m$, let $g:\mathbb{R}^n\to\mathbb{R}^m$ be such that $\Vert{g(x)}\Vert\leq M\Vert{x}\Vert^2$, and let $f(x) = L(x)+g(x)$.Prove that $\mathbf{D}f_0 = L$.
\end{exercise}

First we show that $\mathbf{D}g_0 = 0$, then that $\mathbf{D}L_x = L$ for all $x$, then that $\mathbf{D}f = \mathbf{D}g + \mathbf{D}L$, from which the conclusion follows.

\textbf{Part 1:} To begin, note that by assumption, $\Vert{g(0)}\Vert\leq 0$, so $g(0)=0$. Then taking $0(x-x_0)$ to mean ``the zero function evaluated at $x-x_0$", we have
\begin{align*}
	\lim_{x \to 0} \frac{\Vert{g(x)-g(0)-0(x-0)}\Vert}{\Vert{x-0}\Vert} &= \lim_{x \to 0} \frac{\Vert{g(x)}\Vert}{\Vert{x}\Vert} \\
									       &\leq \lim_{x \to 0} \frac{M \Vert{x}\Vert^2}{\Vert{x}\Vert} \\
									       &= \lim_{x \to 0} M \Vert{x}\Vert \\
									       &= 0,
\end{align*}
Thus $g$ is differentiable at 0 and the derivative of $g$ at 0 is the zero function.

\textbf{Part 2:} Now let $x_0$ be arbitrary, then since $L$ is linear, we have
\begin{align*}
	\lim_{x \to x_0} \frac{\Vert{L(x)-L(x_0)-L(x-x_0)}\Vert}{\Vert{x-x_0}\Vert} &= \lim_{x \to x_0} \frac{\Vert{L(x)-L(x_0)-L(x)+L(x_0)}\Vert}{\Vert{x-x_0}\Vert} \\
											     &= \lim_{x \to x_0} 0 \\
											     &= 0.
\end{align*}
Since $x_0$ was arbitrary, this shows that for all $x$, $L$ is differentiable at $x$ and $\mathbf{D}L_x=L$.

\textbf{Part 3:} For the final major part of the proof, we show that two differentiable functions $\phi,\psi :\mathcal{V}\to\mathcal{W}$ (where $\mathcal{V}$ and $\mathcal{W}$ are normed vector spaces) satisfy $\mathbf{D}(\phi+\psi) = \mathbf{D}\phi + \mathbf{D}\psi$. By the triangle inequality, for all $x_0 \in \mathcal{W}$ we have
\begin{align*}
	&\quad \lim_{x \to x_0} \frac{\Vert{\phi(x)+\psi(x)-\phi(x_0)-\psi(x_0)-\mathbf{D}\phi_{x_0}(x-x_0)-\mathbf{D}\psi_{x_0}(x-x_0)}\Vert}{\Vert{x-x_0}\Vert} \\
	&\leq \lim_{x \to x_0} \frac{\Vert{\phi(x)-\phi(x_0)-\mathbf{D}\phi_{x_0}(x-x_0)}\Vert}{\Vert{x-x_0}\Vert} +\lim_{x \to x_0} \frac{\Vert{\psi(x)-\psi(x_0)-\mathbf{D}\psi_{x_0}(x-x_0)}\Vert}{\Vert{x-x_0}\Vert} \\
	&= 0 + 0 = 0.
\end{align*}
Thus $\mathbf{D}(\phi+\psi)=\mathbf{D}\phi + \mathbf{D}\psi$.

\textbf{Conclusion:} We can apply these three facts to show our desired result. We have
\[
	\mathbf{D}f_0 = \mathbf{D}(g+L)_0 = \mathbf{D}g_0+\mathbf{D}L_0 = 0 + L = L.
\] 

%====================
\begin{exercise}{Pg. 344, Ex. 2}
	Investigate the differentiability of
	\[
		f(x,y) = \frac{xy}{\sqrt{x^2+y^2} } 
	\] at $(0,0)$ if $f(0,0)=0$.
\end{exercise}

If $f$ is differentiable at $(0,0)$, then
\[
	\lim_{x,y \to 0} \frac{\Vert{f(x,y) - f(0,0) - \mathbf{D}f_{(0,0)}(x,y)}\Vert}{\Vert{(x,y)}\Vert} = \lim_{x,y \to 0} \frac{\Vert{f(x,y) - \partial_{x}{f} (0,0)x - \partial_{y}{f} (0,0)y}\Vert}{\Vert{(x,y)}\Vert} 
\] must equal 0. We can evaluate the partial derivatives of $f$. We have
\begin{align*}
	\frac{\partial f}{\partial x} (0,0) &= \lim_{h \to 0} \frac{f(h,0) - f(0,0)}{h} \\
					    &= \lim_{h \to 0} \frac{\frac{h \cdot 0}{h} }{h} \\
					    &= \lim_{h \to 0} 0 \\
					    &= 0.
\end{align*}
Similarly, $\partial_{y}{f} (0,0) = 0$ as well. Thus our original limit is
\[
\lim_{x,y \to 0} \frac{\Vert{f(x,y) - \partial_{x}{f} (0,0)x - \partial_{y}{f} (0,0)y}\Vert}{\Vert{(x,y)}\Vert} = \lim_{x,y \to 0} \frac{\Vert{f(x,y)}\Vert}{\Vert{(x,y)}\Vert} = \lim_{x,y \to 0} \frac{|xy|}{x^2+y^2} .
\] 
We claim that this limit does not exist. We consider the limit as we approach $(0,0)$ along the $x$-axis (i.e. $y$ is fixed at 0). We have
\[
\lim_{x,y \to 0} \frac{|xy|}{x^2 + y^2} = \lim_{x \to 0} \frac{|x \cdot 0|}{x^2} = \lim_{x \to 0} 0 = 0.
\] We now consider the limit as we approach $(0,0)$ along the line $y=x$. We have
\[
\lim_{x,y \to 0} \frac{|xy|}{x^2 + y^2} = \lim_{x \to 0} \frac{x^2}{2x^2} = \lim_{x \to 0} \frac{1}{2} = \frac{1}{2} .
\] These two values do not agree, so the limit does not exist. Since it does not exist, it surely cannot equal 0, so $f$ is \textit{not} differentiable at $(0,0)$.

\end{document}

